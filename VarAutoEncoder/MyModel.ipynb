{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 20:55:47.176773: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-11 20:55:47.662035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence\n",
    "from attrs import define, field\n",
    "from tensorflow import cast, expand_dims\n",
    "from tensorflow.math import reduce_mean, square\n",
    "import tensorflow.keras as K\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (28, 28, 1)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@define(frozen=True)\n",
    "class Config:\n",
    "    enc_conv_layers: int = field(kw_only=True)\n",
    "    enc_conv_filters: Sequence[int] = field(kw_only=True)\n",
    "    enc_conv_kernel_size: Sequence[Sequence[int]] = field(kw_only=True)\n",
    "    enc_conv_strides: Sequence[int] = field(kw_only=True)\n",
    "    latent_dim: int = field(kw_only=True)\n",
    "    dec_conv_t_layers: int = field(kw_only=True)\n",
    "    dec_conv_t_filters: Sequence[int] = field(kw_only=True)\n",
    "    dec_conv_t_kernel_size: Sequence[Sequence[int]] = field(kw_only=True)\n",
    "    dec_conv_t_strides: Sequence[int] = field(kw_only=True)\n",
    "    origin_input_shape: Sequence[int] = field(kw_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(K.layers.Layer):\n",
    "    def __init__(self, config, name=None, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.leaky_relu = K.layers.LeakyReLU()\n",
    "        self.conv_layers = [K.layers.Conv2D(\n",
    "            filters=config.enc_conv_filters[i],\n",
    "            kernel_size=config.enc_conv_kernel_size[i],\n",
    "            strides=config.enc_conv_strides[i],\n",
    "            padding='same',\n",
    "            name=f\"encoder_conv_{i}\",\n",
    "        ) for i in range(config.enc_conv_layers)]\n",
    "        \n",
    "        self.flatten = K.layers.Flatten()\n",
    "        self.latent_layer = K.layers.Dense(config.latent_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = cast(expand_dims(inputs, -1), dtype=\"float32\")\n",
    "        for i in range(self.config.enc_conv_layers):\n",
    "            x = self.conv_layers[i](x)\n",
    "            x = self.leaky_relu(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        output = self.latent_layer(x)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(K.layers.Layer):\n",
    "    def __init__(self, config, name=None, **kwargs):\n",
    "        super(Decoder ,self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.leaky_relu = K.layers.LeakyReLU()\n",
    "        self.conv_t_layers = [K.layers.Conv2DTranspose(\n",
    "            filters=config.dec_conv_t_filters[i],\n",
    "            kernel_size=config.dec_conv_t_kernel_size[i],\n",
    "            strides=config.dec_conv_t_strides[i],\n",
    "            padding=\"same\",\n",
    "            name=f\"decoder_conv_t_{i}\"\n",
    "        ) for i in range(self.config.dec_conv_t_layers)]\n",
    "\n",
    "        shape_before_flatten = np.array(self.config.origin_input_shape[:-1]) // np.prod(self.config.enc_conv_strides)\n",
    "        self.dense_layer = K.layers.Dense(units=np.prod(shape_before_flatten) * self.config.latent_dim, name=\"decoder_expanse\")\n",
    "        self.reshape = K.layers.Reshape(np.append(shape_before_flatten, self.config.latent_dim))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_layer(inputs)\n",
    "        x = self.reshape(x)\n",
    "\n",
    "        for i in range(self.config.dec_conv_t_layers):\n",
    "            x = self.conv_t_layers[i](x)\n",
    "\n",
    "            if i < self.config.dec_conv_t_layers - 1:\n",
    "                x = self.leaky_relu(x)\n",
    "        x = K.layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(K.models.Model):\n",
    "    def __init__(self, config: Config, name: str, *args, **kwargs):\n",
    "        super(AutoEncoder, self).__init__(name, *args, **kwargs)\n",
    "        self.encoder = Encoder(config, trainable=True)\n",
    "        self.decoder = Decoder(config, trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        latent_representation = self.encoder(inputs)\n",
    "        reconstructed_output = self.decoder(latent_representation)\n",
    "\n",
    "        return reconstructed_output * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    enc_conv_layers = 4,\n",
    "    enc_conv_filters = [32, 64, 64, 32], \n",
    "    enc_conv_kernel_size = [(3, 3), (3, 3), (3, 3), (3, 3)], \n",
    "    enc_conv_strides = [1, 2, 2, 1],\n",
    "    latent_dim = 2,\n",
    "    origin_input_shape = INPUT_SHAPE,\n",
    "    dec_conv_t_layers = 4,\n",
    "    dec_conv_t_filters = [64, 32, 32, 1],\n",
    "    dec_conv_t_kernel_size = [(3, 3), (3, 3), (3, 3), (3, 3)],\n",
    "    dec_conv_t_strides = [1, 2, 2, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_ds = K.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = mnist_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nor_x_train = x_train / 256\n",
    "#nor_x_test = x_test / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_loss(y_true, y_pred):\n",
    "    trans_y_true = cast(y_true, dtype=\"float32\")\n",
    "    trans_y_pred = cast(y_pred, dtype=\"float32\")\n",
    "    \n",
    "    return reduce_mean(square(trans_y_true - trans_y_pred), axis=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 20:55:48.416039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.433430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.433661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.434558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.434766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.434934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.761403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.761544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.761657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-11 20:55:48.761747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9666 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(config, name=\"MyAutoencoder\")\n",
    "model.compile(optimizer=K.optimizers.Adam(learning_rate=0.0001), loss=r_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-11 20:56:19.177304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-04-11 20:56:19.721286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2024-04-11 20:56:20.003563: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fcd56ed3ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-04-11 20:56:20.003583: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-04-11 20:56:20.006130: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-04-11 20:56:20.076909: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 9s 3ms/step - loss: 4305.3447 - val_loss: 3676.1355\n",
      "Epoch 2/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3611.7720 - val_loss: 3562.5825\n",
      "Epoch 3/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3525.4531 - val_loss: 3483.6277\n",
      "Epoch 4/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3469.4202 - val_loss: 3435.9688\n",
      "Epoch 5/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3427.7693 - val_loss: 3409.6245\n",
      "Epoch 6/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3393.9316 - val_loss: 3390.1704\n",
      "Epoch 7/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3367.0239 - val_loss: 3349.3911\n",
      "Epoch 8/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3340.1746 - val_loss: 3313.8787\n",
      "Epoch 9/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3319.2656 - val_loss: 3297.9175\n",
      "Epoch 10/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3298.0010 - val_loss: 3262.3943\n",
      "Epoch 11/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3280.7031 - val_loss: 3291.1685\n",
      "Epoch 12/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3261.4795 - val_loss: 3255.8325\n",
      "Epoch 13/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3243.0452 - val_loss: 3234.3604\n",
      "Epoch 14/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3222.4746 - val_loss: 3218.3916\n",
      "Epoch 15/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3207.0728 - val_loss: 3168.6211\n",
      "Epoch 16/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3189.6533 - val_loss: 3179.4441\n",
      "Epoch 17/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3178.5234 - val_loss: 3159.6018\n",
      "Epoch 18/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3162.5203 - val_loss: 3160.0212\n",
      "Epoch 19/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3153.0349 - val_loss: 3141.0857\n",
      "Epoch 20/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3140.3152 - val_loss: 3115.5811\n",
      "Epoch 21/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3130.9111 - val_loss: 3121.3799\n",
      "Epoch 22/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3120.8867 - val_loss: 3113.3601\n",
      "Epoch 23/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3111.0933 - val_loss: 3089.5535\n",
      "Epoch 24/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3102.6538 - val_loss: 3100.7258\n",
      "Epoch 25/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3093.5056 - val_loss: 3064.3103\n",
      "Epoch 26/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3087.0232 - val_loss: 3071.0176\n",
      "Epoch 27/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3076.8333 - val_loss: 3070.8994\n",
      "Epoch 28/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3069.0820 - val_loss: 3064.5391\n",
      "Epoch 29/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3060.4456 - val_loss: 3043.9983\n",
      "Epoch 30/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3054.4995 - val_loss: 3043.7849\n",
      "Epoch 31/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3046.1694 - val_loss: 3025.4939\n",
      "Epoch 32/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3038.2014 - val_loss: 3035.5793\n",
      "Epoch 33/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3032.1099 - val_loss: 3006.3979\n",
      "Epoch 34/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3026.5974 - val_loss: 3014.0979\n",
      "Epoch 35/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3018.0513 - val_loss: 3011.6277\n",
      "Epoch 36/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3010.8037 - val_loss: 3006.6782\n",
      "Epoch 37/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 3004.0994 - val_loss: 3000.7024\n",
      "Epoch 38/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2999.4431 - val_loss: 2980.2454\n",
      "Epoch 39/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2992.0063 - val_loss: 2963.8076\n",
      "Epoch 40/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2985.6655 - val_loss: 2970.5325\n",
      "Epoch 41/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2979.1694 - val_loss: 2968.1243\n",
      "Epoch 42/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2971.9878 - val_loss: 2951.3235\n",
      "Epoch 43/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2968.4236 - val_loss: 2949.9021\n",
      "Epoch 44/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2960.1211 - val_loss: 2950.8269\n",
      "Epoch 45/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2952.4414 - val_loss: 2940.8467\n",
      "Epoch 46/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2947.7834 - val_loss: 2938.8872\n",
      "Epoch 47/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2941.1729 - val_loss: 2916.8120\n",
      "Epoch 48/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2936.3259 - val_loss: 2916.5647\n",
      "Epoch 49/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2929.0688 - val_loss: 2913.3967\n",
      "Epoch 50/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2924.7664 - val_loss: 2910.6790\n",
      "Epoch 51/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2919.3555 - val_loss: 2912.9871\n",
      "Epoch 52/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2912.3000 - val_loss: 2909.7876\n",
      "Epoch 53/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2907.8782 - val_loss: 2895.0381\n",
      "Epoch 54/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2903.4998 - val_loss: 2886.7195\n",
      "Epoch 55/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2895.6626 - val_loss: 2904.2375\n",
      "Epoch 56/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2892.8611 - val_loss: 2898.7068\n",
      "Epoch 57/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2887.7966 - val_loss: 2880.9333\n",
      "Epoch 58/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2886.2852 - val_loss: 2883.9250\n",
      "Epoch 59/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2879.6675 - val_loss: 2865.1033\n",
      "Epoch 60/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2876.4375 - val_loss: 2879.8406\n",
      "Epoch 61/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2872.1252 - val_loss: 2860.3967\n",
      "Epoch 62/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2867.3530 - val_loss: 2863.1211\n",
      "Epoch 63/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2863.3320 - val_loss: 2858.3867\n",
      "Epoch 64/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2858.6555 - val_loss: 2843.0979\n",
      "Epoch 65/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2855.8042 - val_loss: 2874.3142\n",
      "Epoch 66/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2851.7114 - val_loss: 2864.3103\n",
      "Epoch 67/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2850.1238 - val_loss: 2857.3433\n",
      "Epoch 68/100\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2844.8162 - val_loss: 2825.7832\n",
      "Epoch 69/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2841.6790 - val_loss: 2844.6555\n",
      "Epoch 70/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2836.7605 - val_loss: 2840.1821\n",
      "Epoch 71/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2834.5518 - val_loss: 2853.6677\n",
      "Epoch 72/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2830.8843 - val_loss: 2838.9534\n",
      "Epoch 73/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2827.0144 - val_loss: 2834.0210\n",
      "Epoch 74/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2825.1008 - val_loss: 2832.5098\n",
      "Epoch 75/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2821.4805 - val_loss: 2822.5415\n",
      "Epoch 76/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2818.4712 - val_loss: 2814.1824\n",
      "Epoch 77/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2814.1135 - val_loss: 2833.2852\n",
      "Epoch 78/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2810.4844 - val_loss: 2797.8022\n",
      "Epoch 79/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2805.5735 - val_loss: 2840.5056\n",
      "Epoch 80/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2800.0430 - val_loss: 2841.9641\n",
      "Epoch 81/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2799.3955 - val_loss: 2800.9819\n",
      "Epoch 82/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2795.0588 - val_loss: 2801.2034\n",
      "Epoch 83/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2794.3264 - val_loss: 2813.4009\n",
      "Epoch 84/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2789.5696 - val_loss: 2802.1038\n",
      "Epoch 85/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2786.7122 - val_loss: 2789.8726\n",
      "Epoch 86/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2783.0369 - val_loss: 2785.7444\n",
      "Epoch 87/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2780.4458 - val_loss: 2791.2473\n",
      "Epoch 88/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2779.5615 - val_loss: 2817.1572\n",
      "Epoch 89/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2774.4250 - val_loss: 2791.3645\n",
      "Epoch 90/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2769.3647 - val_loss: 2783.9409\n",
      "Epoch 91/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2767.0745 - val_loss: 2790.5437\n",
      "Epoch 92/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2764.6189 - val_loss: 2788.9114\n",
      "Epoch 93/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2760.5837 - val_loss: 2787.6104\n",
      "Epoch 94/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2759.0657 - val_loss: 2781.9900\n",
      "Epoch 95/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2754.7249 - val_loss: 2784.9158\n",
      "Epoch 96/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2754.9250 - val_loss: 2774.2639\n",
      "Epoch 97/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2751.7126 - val_loss: 2817.2905\n",
      "Epoch 98/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2748.8018 - val_loss: 2745.3674\n",
      "Epoch 99/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2744.8420 - val_loss: 2783.6184\n",
      "Epoch 100/100\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2742.1006 - val_loss: 2757.0527\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fd220388970>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,\n",
    "        y=x_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(x_test, x_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_output = model.encoder(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f3584184f70>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.scatter(latent_output[0:][0], latent_output[0:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"auto_encoder_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder_3 (Encoder)         multiple                  80834     \n",
      "                                                                 \n",
      " decoder_3 (Decoder)         multiple                  21145     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 101979 (398.36 KB)\n",
      "Trainable params: 101979 (398.36 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
